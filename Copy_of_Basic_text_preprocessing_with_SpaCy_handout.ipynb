{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Copy_of_Basic_text_preprocessing_with_SpaCy_handout.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMsrdgz73CbI"
      },
      "source": [
        "# Using the SpaCy pipeline\n",
        "\n",
        "This task is aiming to demonstrate the tokenization capabilites of [SpaCy](https://spacy.io/), as well as to serve as an introduction to the pipeline's capabilities combined with [rule based matching](https://spacy.io/usage/rule-based-matching).\n",
        "\n",
        "Our goal will be to process the demonstration text, as well as to correct for some peculiarities, like special pronunciation marks, wide-spread abbreviations and foreign language insertions into our text.\n",
        "\n",
        "It is mandatory, to stick to SpaCy based pipeline operations so as to make our analysis reproducible by running the pipeline on other texts presumably coming from the same corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2ksq0UW3CbQ"
      },
      "source": [
        "## Our demonstration text\n",
        "\n",
        "Original from [Deutsche Sprache](https://de.wikipedia.org/wiki/Deutsche_Sprache) Wikipedia entry - with some modifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6326LBEw907"
      },
      "source": [
        "text= '''Die deutsche Sprache bzw. Deutsch ([dɔʏ̯t͡ʃ]; abgekürzt dt. oder dtsch.) ist eine westgermanische Sprache.\n",
        "\n",
        "And this is an English sentence inbetween.\n",
        "\n",
        "Ihr Sprachraum umfasst Deutschland, Österreich, die Deutschschweiz, Liechtenstein, Luxemburg, Ostbelgien, Südtirol, das Elsass und Lothringen sowie Nordschleswig. Außerdem ist sie eine Minderheitensprache in einigen europäischen und außereuropäischen Ländern, z. B. in Rumänien und Südafrika, sowie Nationalsprache im afrikanischen Namibia.'''"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjfT2ZED3Cbq"
      },
      "source": [
        "## Basic usage\n",
        "\n",
        "After installing SpaCy, let us demonstrate it's basic usage by analysing our text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:04.978496Z",
          "start_time": "2019-11-08T11:44:03.810849Z"
        },
        "id": "e8XVa-KX3Cby"
      },
      "source": [
        "%%capture\n",
        "!pip install tabulate >> /dev/null\n",
        "!pip install spacy"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezEs5d3y3CdG",
        "outputId": "5f48192c-201e-4621-f9cb-60384675bf6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We download the German language models for Spacy\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2KDcjcy3CdM"
      },
      "source": [
        "# Ok, we installed SpaCy, but do we have a model for German?\n",
        "# Something has to be done here to get it!\n",
        "\n",
        "import de_core_news_sm"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.365430Z",
          "start_time": "2019-11-08T11:44:04.981233Z"
        },
        "id": "9Q2A1ZZ_3CdW"
      },
      "source": [
        "# Please do the appropriate imports for SpaCy and it's rule based Matcher class!\n",
        "\n",
        "# import spacy\n",
        "import spacy\n",
        "\n",
        "# Import the Matcher\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Please don't forget to instantiate the language model that we will use later on for analysis\n",
        "nlp = spacy.load('de_core_news_sm')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.385990Z",
          "start_time": "2019-11-08T11:44:06.366784Z"
        },
        "id": "jdEn8lTU3Cdd"
      },
      "source": [
        "# And please use the model to analyse the text from above!\n",
        "doc = nlp(text)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZfedFWU3Cdm"
      },
      "source": [
        "### Helper functions for nice printout\n",
        "\n",
        "We just define some helper functions for nice printout. Nothing to do here, except to observe the ways one can iterate over a corpus or sentence, as well as the nice output of [Tabulate](https://bitbucket.org/astanin/python-tabulate/src/master/). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.395057Z",
          "start_time": "2019-11-08T11:44:06.387362Z"
        },
        "id": "cpwnuXA43Cdn"
      },
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "def print_sentences(doc):\n",
        "    for sentence in doc.sents:\n",
        "        print(sentence,\"\\n\")\n",
        "\n",
        "def print_tokens_for_sentence(doc,sentence_num, stopwords=False):\n",
        "    attribs=[]\n",
        "    for token in list(doc.sents)[sentence_num]:\n",
        "        if token.has_extension(\"is_lemma_stop\"):\n",
        "            if stopwords and token._.is_lemma_stop:\n",
        "                pass\n",
        "            else:\n",
        "                attribs.append([token.text, token.lemma_, token.pos_])\n",
        "        else:\n",
        "            attribs.append([token.text, token.lemma_, token.pos_])\n",
        "    print(tabulate(attribs))\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGUCjIQd3BEx",
        "outputId": "8831dfae-8085-460c-b6a4-628460426ce2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#lets check how many sentences we have in our text\n",
        "j = 0\n",
        "for i in doc.sents:\n",
        "  j=j+1\n",
        "print('We have ' + str(j)+ ' sentences in text')  "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 5 sentences in text\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.401099Z",
          "start_time": "2019-11-08T11:44:06.397107Z"
        },
        "id": "g-535s3m3Cdu",
        "outputId": "ee4ceac9-d251-45b9-f3bc-d433726b8d52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print_sentences(doc)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Die deutsche Sprache bzw. Deutsch ([dɔʏ̯t͡ʃ]; abgekürzt dt. \n",
            "\n",
            "oder dtsch.) ist eine westgermanische Sprache.\n",
            "\n",
            " \n",
            "\n",
            "And this is an English sentence inbetween.\n",
            "\n",
            " \n",
            "\n",
            "Ihr Sprachraum umfasst Deutschland, Österreich, die Deutschschweiz, Liechtenstein, Luxemburg, Ostbelgien, Südtirol, das Elsass und Lothringen sowie Nordschleswig. \n",
            "\n",
            "Außerdem ist sie eine Minderheitensprache in einigen europäischen und außereuropäischen Ländern, z. B. in Rumänien und Südafrika, sowie Nationalsprache im afrikanischen Namibia. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.410817Z",
          "start_time": "2019-11-08T11:44:06.404499Z"
        },
        "id": "hEX3Vtdn3Cd8",
        "outputId": "f0b9ec96-bc5e-4d8f-f546-650df6bab4b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print_tokens_for_sentence(doc,1)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------  ---------------  -----\n",
            "oder             oder             CCONJ\n",
            "dtsch            dtsch            ADJ\n",
            ".                .                PUNCT\n",
            ")                )                PUNCT\n",
            "ist              sein             AUX\n",
            "eine             einen            DET\n",
            "westgermanische  westgermanische  ADJ\n",
            "Sprache          Sprache          NOUN\n",
            ".                .                PUNCT\n",
            "                                  SPACE\n",
            "---------------  ---------------  -----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98MmBQiF3CeG"
      },
      "source": [
        "## Matching \"zum Beispiel\"\n",
        "\n",
        "We are a bit frustrated, that the standard analysis pipeline does not know, that in German, \"z. B.\" is the abbreviation of \"zum Beispiel\" (like eg. is for \"for example\"), thus we would like to correct this.\n",
        "\n",
        "Our approach is to extend the pipeline and do a matching, whereby we replace the `lemma` form of \"z. B.\" to the appropriate long form.\n",
        "\n",
        "**IMPORTANT** design principle by SpaCy is, that one **always keeps the possibility to restore the original text**, so we are **NOT to modify `token.text`**. In the analysed form, we can do whatever we want.\n",
        "\n",
        "It is typical to add layers to the pipeline which modify the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tchwPz303CeH"
      },
      "source": [
        "For our purposes, we will use rule based matching to achieve our goals.\n",
        "\n",
        "A detailed description on rule based matching in SpaCy can be found [here](https://spacy.io/usage/rule-based-matching), or [here](https://medium.com/@ashiqgiga07/rule-based-matching-with-spacy-295b76ca2b68)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBcjgRSp3CeI"
      },
      "source": [
        "### Build the matcher\n",
        "\n",
        "With the help of rule based matching we create a matcher that reacts to the presence of \"z. B.\" exactly, then we use this matcher to define a pipeline step, that after matching, replaces the lemmas of the tokens \"z.\" and \"B.\" to  their full written equivalent.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.417129Z",
          "start_time": "2019-11-08T11:44:06.412535Z"
        },
        "id": "N5QtPd0X3CeJ"
      },
      "source": [
        "zb_matcher = Matcher(nlp.vocab, validate=True) # Please instantiate a matcher with the appropriate parameters - think about all the words of the corpus...\n",
        "pattern = [{\"LOWER\": \"z.\"}, {\"TEXT\": \"B.\"}] \n",
        "\n",
        "zb_matcher.add(\"z.B. pattern\", None, pattern) # Please add an appropriate pattern to the matcher to match \"z. B.\"\n",
        "\n",
        "def zb_replacer(doc):\n",
        "    matched_spans = []\n",
        "    # Please use the matcher to get matches!\n",
        "    matches = zb_matcher(doc)\n",
        "    # Plsease iterate over the matches!\n",
        "    for match_id, start, end in matches:\n",
        "        span = doc[start:end] # get the span of text based on the matches coordinates!\n",
        "        matched_spans.append(span)\n",
        "        print(\"ZB MATCH!!!\")\n",
        "\n",
        "    # Please iterate over matched spans\n",
        "    for i in matched_spans:  \n",
        "        # And replace their lemmas to the appropriate ones!\n",
        "        # Please observe, that you don't have the ID of the desired lemmas, just the their string form.\n",
        "        i[0].lemma_ = 'zum'\n",
        "        i[1].lemma_ = 'Beispiel'\n",
        "    return doc"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_k4-2rn3CeP"
      },
      "source": [
        "### Register it to the pipeline\n",
        "\n",
        "After creating this processing step, we register it to be part of the pipeline and then run our analysis again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.421259Z",
          "start_time": "2019-11-08T11:44:06.418628Z"
        },
        "id": "HQnqVex63CeP"
      },
      "source": [
        "# Plase register the new zb_replacer to the pipeline!\n",
        "# Think about, where to place it!\n",
        "nlp.add_pipe(zb_replacer) "
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO7rjgWV3CeX"
      },
      "source": [
        "### Re-do the analysis and observe results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.442000Z",
          "start_time": "2019-11-08T11:44:06.422574Z"
        },
        "id": "LHMCi0uA3CeY",
        "outputId": "b9667e63-4d5d-4279-85dc-c4c90f8b49aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "doc=nlp(text)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ZB MATCH!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.448772Z",
          "start_time": "2019-11-08T11:44:06.443720Z"
        },
        "id": "Mv2Zn06q3Cee",
        "outputId": "47b68677-cab0-407f-997c-a71a6a19e501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print_tokens_for_sentence(doc,-1)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------  -------------------  -----\n",
            "Außerdem             Außerdem             ADV\n",
            "ist                  sein                 AUX\n",
            "sie                  ich                  PRON\n",
            "eine                 einen                DET\n",
            "Minderheitensprache  Minderheitensprache  NOUN\n",
            "in                   in                   ADP\n",
            "einigen              einig                DET\n",
            "europäischen         europäisch           ADJ\n",
            "und                  und                  CCONJ\n",
            "außereuropäischen    außereuropäisch      ADJ\n",
            "Ländern              Land                 NOUN\n",
            ",                    ,                    PUNCT\n",
            "z.                   zum                  ADP\n",
            "B.                   Beispiel             NOUN\n",
            "in                   in                   ADP\n",
            "Rumänien             Rumänien             PROPN\n",
            "und                  und                  CCONJ\n",
            "Südafrika            Südafrika            PROPN\n",
            ",                    ,                    PUNCT\n",
            "sowie                sowie                CCONJ\n",
            "Nationalsprache      Nationalsprache      NOUN\n",
            "im                   im                   ADP\n",
            "afrikanischen        afrikanisch          ADJ\n",
            "Namibia              Namibia              NOUN\n",
            ".                    .                    PUNCT\n",
            "-------------------  -------------------  -----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNwWyS-93Cek"
      },
      "source": [
        "## What are those ugly pronunciation signs doing there?\n",
        "\n",
        "OK, so far so good. Let's observe, what is the problem with the first sentence!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.473528Z",
          "start_time": "2019-11-08T11:44:06.450123Z"
        },
        "id": "vwCY7mQl3Cel",
        "outputId": "20578edb-28a6-435f-ae92-3a927626f79f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "doc=nlp(text)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ZB MATCH!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.484081Z",
          "start_time": "2019-11-08T11:44:06.476136Z"
        },
        "id": "ynoIcaHo3Ceq",
        "outputId": "3c9536d1-40cc-4670-c45b-126b2faaeed5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print_tokens_for_sentence(doc,0)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------  ---------------  -----\n",
            "Die        der              DET\n",
            "deutsche   deutsch          ADJ\n",
            "Sprache    Sprache          NOUN\n",
            "bzw.       beziehungsweise  CCONJ\n",
            "Deutsch    Deutsch          NOUN\n",
            "(          (                PUNCT\n",
            "[          [                NOUN\n",
            "dɔʏ̯t͡ʃ      dɔʏ̯t͡ʃ            PROPN\n",
            "]          ]                ADP\n",
            ";          ;                PUNCT\n",
            "abgekürzt  abkürzen         VERB\n",
            "dt         dt               X\n",
            ".          .                PUNCT\n",
            "---------  ---------------  -----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCi9beIy3Cew"
      },
      "source": [
        "As we can see, poor pipeline can not really cope with the pronunciation markings of the phonetic alphabet, and thus thinks, that the signs are representing a foreign proper noun. \n",
        "\n",
        "We would like to remedy this, and since we do expect further texts from the corpus to contain these inserted phonetics, we would like to match, merge and replace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8FvtMc93Cex"
      },
      "source": [
        "## Building up matcher for PRONUNCIATION\n",
        "\n",
        "To be more specific, we again first build up a matcher, that aims at the \"square brackets\" markings around the pronunciation. The task is to match everything between square brackets, or to be more specific: **everything that starts with an opening square bracket, and finishes with \";\"**.\n",
        "\n",
        "This matcher can then be used to:\n",
        "\n",
        "1. Merge the resulting matching `span` into one token\n",
        "2. Replace the token's lemma to \"PRONUNCIATION\"\n",
        "\n",
        "For this to be achievable, we have to first register \"PRONUNCIATION\" as part of the vocabulary, moreover mark it as [\"stopword\"](https://en.wikipedia.org/wiki/Stop_words). (More on SpaCy's stopword handling [here](https://medium.com/@makcedward/nlp-pipeline-stop-words-part-5-d6770df8a936)) See below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.497568Z",
          "start_time": "2019-11-08T11:44:06.486489Z"
        },
        "id": "iAnvHRo53Cey"
      },
      "source": [
        "# Please instantiate and build the matcher as before with the appropriate pattern!\n",
        "# Make it so, that the pattern will match ALL future pronunciations, not just the present one!\n",
        "pronunciation_matcher = Matcher(nlp.vocab, validate=True) \n",
        "pattern = [{\"IS_PUNCT\": True},\n",
        "           {\"IS_LOWER\": True},\n",
        "           {\"IS_PUNCT\": True},\n",
        "           {\"IS_PUNCT\": True}]\n",
        "pronunciation_matcher.add(\"pronunciation pattern\", None, pattern)\n",
        "\n",
        "# We set the properties for the new word \"PRONUNCIATION\"\n",
        "lex = nlp.vocab['PRONUNCIATION']\n",
        "lex.is_oov = False\n",
        "lex.is_stop = True\n",
        "\n",
        "def pronunciation_replacer(doc):\n",
        "    \n",
        "    # Using the template above, please build a pronunciation replacer, that\n",
        "\n",
        "    # 1. gets the matches\n",
        "    # 2. merges them into one\n",
        "    matched_spans = []\n",
        "    matches = pronunciation_matcher(doc)\n",
        "    for match_id, start, end in matches:\n",
        "      span = doc[start:end] \n",
        "      matched_spans.append(span)\n",
        "      print(\"Pronunciation Match\")\n",
        "    \n",
        "    # 3. Replaces their lemma string and lemma ID\n",
        "    # 4. sets it's POS to \"NOUN\"\n",
        "    merged_span = span.merge()\n",
        "    merged_span.lemma_ = \"PRONUNCIATION\"\n",
        "    merged_span.pos_ = \"NOUN\"\n",
        "    \n",
        "    return doc\n",
        "\n",
        "\n",
        "nlp.add_pipe(pronunciation_replacer, after=\"zb_replacer\") "
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3-x2Ahl3Ce5"
      },
      "source": [
        "### Observing result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.623259Z",
          "start_time": "2019-11-08T11:44:06.500096Z"
        },
        "id": "sUSUgmGG3Ce6",
        "outputId": "4130e79c-0904-4f4d-bb2f-97aecd48bc3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "doc=nlp(text)\n",
        "print_tokens_for_sentence(doc,0)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ZB MATCH!!!\n",
            "Pronunciation Match\n",
            "---------  ---------------  -----\n",
            "Die        der              DET\n",
            "deutsche   deutsch          ADJ\n",
            "Sprache    Sprache          NOUN\n",
            "bzw.       beziehungsweise  CCONJ\n",
            "Deutsch    Deutsch          NOUN\n",
            "(          (                PUNCT\n",
            "[dɔʏ̯t͡ʃ];   PRONUNCIATION    NOUN\n",
            "abgekürzt  abkürzen         VERB\n",
            "dt         dt               X\n",
            ".          .                PUNCT\n",
            "---------  ---------------  -----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjPcNTSi3CfC"
      },
      "source": [
        "In the future, we decide, we would not want to include the pronunciation tokens in our view. So we have to mark them as wtopwords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU9n2xoI3CfD"
      },
      "source": [
        "### Registering PRONUNCIATION as a stopword\n",
        "\n",
        "Stopwords are typically those words, which do not contribute to the meaning of the sentence, are just there for syntactic reasons. There is a vague running list of these for languages. We will use and extend the German one in SpaCy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.627929Z",
          "start_time": "2019-11-08T11:44:06.625286Z"
        },
        "id": "z2LVCA4-3CfE"
      },
      "source": [
        "# import stop words from GERMAN language data\n",
        "from spacy.lang.de.stop_words import STOP_WORDS\n",
        "\n",
        "# Add PRONUNCIATION to stopwords\n",
        "STOP_WORDS.add(\"PRONUNCIATION\")"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3TTCCiGhuhx",
        "outputId": "3683ca82-df1d-475e-a845-0fa8d4bc28ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp.pipeline"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f85b9e96f98>),\n",
              " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f85b43316a8>),\n",
              " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f85b4331708>),\n",
              " ('zb_replacer', <function __main__.zb_replacer>),\n",
              " ('pronunciation_replacer', <function __main__.pronunciation_replacer>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SH3jkYF3CfI"
      },
      "source": [
        "But since we will only be able to manipulate the lemmas of the pronunciation markings, we would have to let SpaCy know, that - in contrast to the default behavior, where stopwords are filtered on `text` level, we would like to have a new property for words, that is based on `lemma` level stopword filtering.\n",
        "\n",
        "For these we will use extensions!\n",
        "\n",
        "For more info please see [here](https://spacy.io/api/token#set_extension)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.635721Z",
          "start_time": "2019-11-08T11:44:06.629472Z"
        },
        "id": "8AROq9633CfJ"
      },
      "source": [
        "from spacy.tokens import Token\n",
        "\n",
        "# Please define a function (or lambda expression!) that checks if a Token, or its lower case for, \n",
        "# OR it's lemma string is contained it he stopword list above.\n",
        "stop_words_getter = lambda token: token.is_stop or token.lower_ in STOP_WORDS or token.lemma_ in STOP_WORDS\n",
        "\n",
        "# Set the above defined function as a extension for Token under the name \"is_lemma_stop\" as a getter!\n",
        "Token.set_extension('is_lemma_stop', getter=stop_words_getter, force=True)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.672287Z",
          "start_time": "2019-11-08T11:44:06.637068Z"
        },
        "id": "ZQCRsaUf3CfO",
        "outputId": "b7b42eb7-9dbc-49e9-cf74-d3a9d0503d9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "doc=nlp(text)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ZB MATCH!!!\n",
            "Pronunciation Match\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:06.677941Z",
          "start_time": "2019-11-08T11:44:06.673660Z"
        },
        "id": "545qQ0jy3CfT",
        "outputId": "d596882f-5b75-46c1-c96b-a0af1b4c9f31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print_tokens_for_sentence(doc,0, stopwords=True)\n",
        "\n",
        "assert len(list(doc.sents)[0]) == 10"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------  ---------------  -----\n",
            "deutsche   deutsch          ADJ\n",
            "Sprache    Sprache          NOUN\n",
            "bzw.       beziehungsweise  CCONJ\n",
            "Deutsch    Deutsch          NOUN\n",
            "(          (                PUNCT\n",
            "abgekürzt  abkürzen         VERB\n",
            "dt         dt               X\n",
            ".          .                PUNCT\n",
            "---------  ---------------  -----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqjP4U4-3CfZ"
      },
      "source": [
        "## Language detection\n",
        "\n",
        "We could also observe, that there is some English text inbetween our nice German sentences. We would like to detect foreign sentences and by later processing, ignore / skip them.\n",
        "\n",
        "For this to be achievable, we need some language detection capabilities.\n",
        "\n",
        "Luckily enough, we can make it part of our pipeline via [this extension](#https://spacy.io/universe/project/spacy-langdetect)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32eBl8i83Cfa"
      },
      "source": [
        "### Standard installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:07.709209Z",
          "start_time": "2019-11-08T11:44:06.679245Z"
        },
        "id": "9H5V-z_J3Cfb"
      },
      "source": [
        "%%capture\n",
        "!pip install spacy-langdetect"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:07.732209Z",
          "start_time": "2019-11-08T11:44:07.714434Z"
        },
        "id": "GZu-rSup3Cfh"
      },
      "source": [
        "#Please import the language detector!\n",
        "from spacy_langdetect import LanguageDetector"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iliO72i93Cfm"
      },
      "source": [
        "### Adding language detection to our pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:07.736304Z",
          "start_time": "2019-11-08T11:44:07.733828Z"
        },
        "id": "OHBQsuT23Cfn"
      },
      "source": [
        "# Please register it to the pipeline as the final step of processing!\n",
        "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok_pSPAjxe03",
        "outputId": "dea2c90e-fbd1-437c-c11c-0abbf4a8b0b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nlp.pipeline"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f85b9e96f98>),\n",
              " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f85b43316a8>),\n",
              " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x7f85b4331708>),\n",
              " ('zb_replacer', <function __main__.zb_replacer>),\n",
              " ('pronunciation_replacer', <function __main__.pronunciation_replacer>),\n",
              " ('language_detector',\n",
              "  <spacy_langdetect.spacy_langdetect.LanguageDetector at 0x7f85b828c438>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEzyY1WO3Cfr"
      },
      "source": [
        "### Observing results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:07.776361Z",
          "start_time": "2019-11-08T11:44:07.737797Z"
        },
        "id": "SPyaHt5B3Cfs",
        "outputId": "dc15f3b0-a1f9-4b86-af50-ab25221b61ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "doc = nlp(text)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ZB MATCH!!!\n",
            "Pronunciation Match\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PMtmve4uYAe",
        "outputId": "9310d74f-bae8-4006-928c-fb6f31c1b916",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attribs = []\n",
        "for sentence in doc.sents:\n",
        "    attribs.append([list(sentence)[:15],\"...\", sentence._.language])\n",
        "print(tabulate(attribs))\n",
        "\n",
        "# Please observe how one accesses anextension!!"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------------------------------------------  ---  -----------------------------------------------\n",
            "[Die, deutsche, Sprache, bzw., Deutsch, (, [dɔʏ̯t͡ʃ];, abgekürzt, dt, .]                                                         ...  {'language': 'de', 'score': 0.9999982539775079}\n",
            "[oder, dtsch, ., ), ist, eine, westgermanische, Sprache, .,                                                                    ...  {'language': 'de', 'score': 0.9999940650974677}\n",
            "\n",
            "]\n",
            "[And, this, is, an, English, sentence, inbetween, .,                                                                           ...  {'language': 'en', 'score': 0.9999972134255091}\n",
            "\n",
            "]\n",
            "[Ihr, Sprachraum, umfasst, Deutschland, ,, Österreich, ,, die, Deutschschweiz, ,, Liechtenstein, ,, Luxemburg, ,, Ostbelgien]  ...  {'language': 'de', 'score': 0.9999962650559461}\n",
            "[Außerdem, ist, sie, eine, Minderheitensprache, in, einigen, europäischen, und, außereuropäischen, Ländern, ,, z., B., in]     ...  {'language': 'de', 'score': 0.9999959982493847}\n",
            "-----------------------------------------------------------------------------------------------------------------------------  ---  -----------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpjR81Kj3Cf1"
      },
      "source": [
        "# Creating final generator for cleaned text\n",
        "\n",
        "Typically for a later stage of NLP, we would like to have a generator like function, which allows us to iteratively access the corpus, albeit in it's cleaned and encoded form. Integer encoding (as well as one hot encoding) are quite typical representations of text.\n",
        "\n",
        "In this spirit, we would like to implement a generator, that gives back an **array of lemmas OR lemma IDs for each sentence in the corpus, filtering out non-German sentences and punctuation / space marks**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-08T11:44:08.105282Z",
          "start_time": "2019-11-08T11:44:08.101443Z"
        },
        "id": "i6imC5mY3Cf2"
      },
      "source": [
        "# Please implement a generator function that yields the text of the corpus as lists of sentences\n",
        "# Based on the parameters either as a list of strings or a list of IDs\n",
        "# It should filter out non-German sentences\n",
        "# as well as topwords based on lemmas\n",
        "# and punctuation and \"space like\" characters!\n",
        "\n",
        "def sentence_generator(doc, ids=False):\n",
        "  stopwords = True\n",
        "  \n",
        "  for sentence in doc.sents:\n",
        "    # Create a list that stores token.lemma_\n",
        "    list_token_lemma=[]\n",
        "    # Create a list that stores token.lemma (ids)\n",
        "    list_token_lemma_ids=[]\n",
        "    #go forward only if language of sentence is German\n",
        "    if sentence._.language['language'] ==\"de\":\n",
        "      # iterate over the tokens in the sentence, so that we can\n",
        "      # remove unwanted tokens\n",
        "      for token in list(sentence):\n",
        "        # if we have stopwords on lemma level(extentions) and text level, PASS\n",
        "        if stopwords and token._.is_lemma_stop:\n",
        "          pass\n",
        "        # if POS is not a PUNCTUATION and its not SPACE, append the \n",
        "        # respective lists for lemma and lemma ids\n",
        "        elif token.pos_ != 'PUNCT' and token.pos_ != 'SPACE':\n",
        "          list_token_lemma.append(token.lemma_)\n",
        "          list_token_lemma_ids.append(token.lemma)\n",
        "      \n",
        "      if ids == True:\n",
        "        yield list_token_lemma_ids\n",
        "      else:\n",
        "        yield list_token_lemma\n",
        "        "
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci1dcmb-6IWR"
      },
      "source": [
        "We had 5 sentenences out of which 1 was english which should be removed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx4W33WyiwFO",
        "outputId": "8942de5c-352e-4fb7-9f7d-206f825c725c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in sentence_generator(doc):\n",
        "    print(i,\"\\n\")\n",
        "\n",
        "for i in sentence_generator(doc, ids=True):\n",
        "    print(i,\"\\n\")\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['deutsch', 'Sprache', 'beziehungsweise', 'Deutsch', 'abkürzen', 'dt'] \n",
            "\n",
            "['dtsch', 'westgermanische', 'Sprache'] \n",
            "\n",
            "['Sprachraum', 'umfasst', 'Deutschland', 'Österreich', 'Deutschschweiz', 'Liechtenstein', 'Luxemburg', 'Ostbelgien', 'Südtirol', 'Elsass', 'Lothringen', 'Nordschleswig'] \n",
            "\n",
            "['Minderheitensprache', 'europäisch', 'außereuropäisch', 'Land', 'Beispiel', 'Rumänien', 'Südafrika', 'Nationalsprache', 'afrikanisch', 'Namibia'] \n",
            "\n",
            "[5968319817064592459, 8431935777423264011, 16143637279988465102, 13347145995516113707, 12068858602874567954, 5135506797272647618] \n",
            "\n",
            "[2552743035069842888, 7654685629011980891, 8431935777423264011] \n",
            "\n",
            "[11854469037278879099, 7289263729939212449, 3491614202785599281, 16047064563126251420, 3469156011154928224, 10833980334450146958, 15216956676957942053, 14493420987399493547, 14425170055224073740, 14854674721094831692, 5682654018506929560, 10694615845175474381] \n",
            "\n",
            "[13853446524293058697, 512110525822973470, 15751849195492229329, 731233208058718707, 176351906757609250, 16018282812866072734, 14398131728093720111, 13884865873598079458, 9226656959411645728, 2911802427415368037] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}